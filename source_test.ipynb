{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_pleQMN3Pmuv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = 'your_api_key'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HcVaOdWYPmw7"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma, ElasticKnnSearch\n",
        "from langchain.embeddings import OpenAIEmbeddings, cohere as ce\n",
        "from langchain.text_splitter import (\n",
        "    RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
        ")\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import RetrievalQA, ConversationChain\n",
        "from langchain.document_loaders import (\n",
        "    TextLoader, DirectoryLoader, PyPDFLoader\n",
        ")\n",
        "from langchain.prompts import (\n",
        "    PromptTemplate, SystemMessagePromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
        ")\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.memory import (\n",
        "    ChatMessageHistory, ConversationBufferMemory, ConversationBufferWindowMemory\n",
        ")\n",
        "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
        "from langchain.prompts import MessagesPlaceholder\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "O38X7EYyPmzh"
      },
      "outputs": [],
      "source": [
        "# PyPDFLoader를 사용하여 'investment.pdf' 파일을 로드합니다.\n",
        "loader = PyPDFLoader('/content/drive/MyDrive/investment.pdf')\n",
        "# 로더를 사용하여 PDF 파일의 내용을 documents 변수에 로드합니다.\n",
        "documents = loader.load()\n",
        "# 'CharacterTextSplitter'를 사용하여 텍스트를 크기 1000의 청크로 분할합니다. 청크 간의 겹침은 없습니다.\n",
        "text_splitter = CharacterTextSplitter(chunk_size = 1000, chunk_overlap = 0)\n",
        "# 분할자를 사용하여 'documents'를 청크로 분할하고 결과를 texts 변수에 저장합니다.\n",
        "texts = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9cuOBOULRoiz"
      },
      "outputs": [],
      "source": [
        "#벡터 데이터베이스가 저장될 디렉토리의 이름을 변수에 저장합니다.\n",
        "persist_directory = 'db'\n",
        "# 텍스트 문서의 임베딩을 생성하는데 사용할 모델을 변수에 할당합니다.\n",
        "embedding = OpenAIEmbeddings()\n",
        "# Chroma 클래스의 'from_documents' 메서드를 호출하여 문서의 임베딩을 생성하고 벡터데이터 베이스를 초기화합니다.\n",
        "vectordb = Chroma.from_documents(\n",
        "    documents=documents,\n",
        "    embedding=embedding,\n",
        "    persist_directory=persist_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aVdWRzH0Rok8"
      },
      "outputs": [],
      "source": [
        "# persist 메서드를 호출하여 'vectordb'에서 생성된 벡터 데이터베이스를 디스크에 저장합니다.\n",
        "# 이렇게 하면 나중에 데이터베이스를 다시 로드하여 사용할 수 있습니다.\n",
        "vectordb.persist()\n",
        "# 메모리에서 데이터베이스를 제거합니다.\n",
        "vectordb = None\n",
        "# Chroma 클래스를 인스턴스화하여 vectordb 변수에 할당합니다. 문서의 임베딩을 저장하고 검색하는데 사용됩니다.\n",
        "vectordb = Chroma(\n",
        "    persist_directory=persist_directory,\n",
        "    embedding_function=embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qa93LUpRRonC",
        "outputId": "51afe6d4-a504-46d1-abb2-d862e2664bff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22\n",
            "30\n",
            "32\n",
            "31\n"
          ]
        }
      ],
      "source": [
        "# 'as_retriever'메서드를 호출하여 'vectordb'에서 생성된 벡터 데이터베이스를 검색가능한 형태로 변환합니다.\n",
        "retriever = vectordb.as_retriever()\n",
        "# 질문과 관련된 문서를 검색합니다.(기본 k값은 4로 설정되어있어, 관련도가 높은 4개의 문서를 검색합니다.)\n",
        "docs = retriever.get_relevant_documents(\"2023년 상반기 금리는어떘어?\")\n",
        "# 각 문서의 페이지 번호를 가져와 출력합니다.\n",
        "for doc in docs:\n",
        "    print(doc.metadata[\"page\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Suggested Question Generator"
      ],
      "metadata": {
        "id": "G-JostZP7qGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Suggested Question Generator\n",
        "\n",
        "prompt_template_generator = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "you are competent financial advisor. You need to create a suggested question,\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Don't answer with short answers, answer with complete sentences.\n",
        "Answer in Korean:\"\"\"\n",
        "# PROMPT에 'context'와 'question'을 입력 변수로하는 프롬프트 템플릿을 저장합니다.\n",
        "PROMPT_generator = PromptTemplate(\n",
        "    template=prompt_template_generator, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "# 'chain_type_kwargs'변수에 딕셔너리를 저장합니다.\n",
        "chain_type_kwargs_generator = {\"prompt\": PROMPT_generator}\n",
        "# 'as_retriever'메서드를 호출하여 'vectordb'에서 생성된 벡터 데이터베이스를 검색 가능한 형태로 변환합니다.\n",
        "# 유사도가 높은 상위 3개의 문서만 반환하도록 지정합니다.\n",
        "retriever_generator = vectordb.as_retriever(search_kwargs={\"k\": 3})\n",
        "# 'gpt-3.5-turbo-16k'모델을 사용합니다.\n",
        "# temperature값이 낮은수록 더 일관성 있는 문장이 생성됩니다.\n",
        "llm_generator = ChatOpenAI(temperature=0.0,model_name='gpt-3.5-turbo-16k')\n",
        "\n",
        "# 체인타입을 결정합니다.\n",
        "qa_chain_generator = RetrievalQA.from_chain_type(\n",
        "    # 언어모델을 할당합니다.\n",
        "    llm=llm_generator,\n",
        "    # 체인의 유형을 지정합니다.\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever_generator,\n",
        "    # 응답과 함께 소스 문서를 반환합니다.\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs=chain_type_kwargs_generator)\n",
        "\n",
        "# 언어 모델의 응답을 처리하는 함수를 정의합니다.\n",
        "def process_llm_response_generator(llm_response_generator):\n",
        "    # 언어 모델의 응답 결과를 출력합니다.\n",
        "    return(llm_response_generator['result'])\n",
        "\n",
        "question = ['올해 금리관련해서 질문 하나만 만들어줄 수 있어?', '올해 경기관련해서 질문 하나만 만들어줄 수 있어?', '은퇴후,건강보험료 관련해서 질문 하나만 만들어줄 수 있어?',\\\n",
        "            '인컴투자 관련해서 질문 하나만 만들어줄 수 있어?','투자원칙 관련해서 질문 하나만 만들어줄 수 있어?', '적립식투자 관련해서 질문 하나만 만들어줄 수 있어?']\n",
        "answer = []\n",
        "for i in range(6) :\n",
        "  result = process_llm_response_generator(llm_response_generator = qa_chain_generator(question[i]))\n",
        "  answer.append(result)\n",
        "answer\n",
        "\n",
        "num1 = random.randrange(0, 3)\n",
        "num2 = random.randrange(4, 6)\n",
        "\n",
        "suggested_question_list = []\n",
        "suggested_question_list.append(answer[num1])\n",
        "suggested_question_list.append(answer[num2])\n",
        "suggested_question_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjdeZJQb7pxN",
        "outputId": "2fc0933e-20da-47c8-d68d-d253646f01ac"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['은퇴 후 건강보험료를 줄이기 위해 어떤 방법을 사용할 수 있을까요?', '2023년에 명심할 투자 원칙은 무엇인가요?']"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MindMap"
      ],
      "metadata": {
        "id": "h6-iY1rYXaJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mind Map\n",
        "\n",
        "prompt_template_mindmap = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "you are competent financial advisor.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer in Korean\n",
        "소주제'들은 주제의 하위주제들로 네가 선정해줘.  '내용'은 문장형식으로, 소주제에 대한 구체적인 너의 설명을 담고 있어야해.  또한 '내용'은 반드시 '~입니다'와 같이 끝나야하고, '소주제'에 대해서 설명하는 어투여야해. let's think step by step.  다음과 같은 형식으로 결과물을 반환해줘. 결과물 이외의 다른 말은 하지마.\n",
        "\n",
        "'''\n",
        "small_subject = [소주제1, 소주제2, 소주제3, 소주제4]\n",
        "contents = [['소주제1'와 관련된 내용1, '소주제1'와 관련된 내용2,'소주제1'와 관련된 내용3],['소주제2'와 관련된 내용1, '소주제2'와 관련된 내용2,'소주제2'와 관련된 내용3],['소주제3'와 관련된 내용1, '소주제3'와 관련된 내용2,'소주제3'와 관련된 내용3],['소주제4'와 관련된 내용1, '소주제4'와 관련된 내용2,'소주제4'와 관련된 내용3]]\n",
        "'''\n",
        "\n",
        "\"\"\"\n",
        "# PROMPT에 'context'와 'question'을 입력 변수로하는 프롬프트 템플릿을 저장합니다.\n",
        "PROMPT_mindmap = PromptTemplate(\n",
        "    template=prompt_template_mindmap, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "# 'chain_type_kwargs'변수에 딕셔너리를 저장합니다.\n",
        "chain_type_kwargs_mindmap = {\"prompt\": PROMPT_mindmap}\n",
        "# 'as_retriever'메서드를 호출하여 'vectordb'에서 생성된 벡터 데이터베이스를 검색 가능한 형태로 변환합니다.\n",
        "# 유사도가 높은 상위 3개의 문서만 반환하도록 지정합니다.\n",
        "retriever_mindmap = vectordb.as_retriever(search_kwargs={\"k\": 3})\n",
        "# 'gpt-3.5-turbo-16k'모델을 사용합니다.\n",
        "# temperature값이 낮은수록 더 일관성 있는 문장이 생성됩니다.\n",
        "llm_mindmap = ChatOpenAI(temperature=0.0,model_name='gpt-3.5-turbo-16k')\n",
        "\n",
        "# 체인타입을 결정합니다.\n",
        "qa_chain_mindmap = RetrievalQA.from_chain_type(\n",
        "    # 언어모델을 할당합니다.\n",
        "    llm=llm_mindmap,\n",
        "    # 체인의 유형을 지정합니다.\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever_mindmap,\n",
        "    # 응답과 함께 소스 문서를 반환합니다.\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs=chain_type_kwargs_mindmap)\n",
        "\n",
        "# 언어 모델의 응답을 처리하는 함수를 정의합니다.\n",
        "def process_llm_response_mindmap(llm_response_mindmap):\n",
        "    # 언어 모델의 응답 결과를 출력합니다.\n",
        "    return(llm_response_mindmap['result'])"
      ],
      "metadata": {
        "id": "NXTphwPdWBE5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"ETF 투자에 대해서 설명해줄래?\"\n",
        "result = process_llm_response_mindmap(qa_chain_mindmap(query))\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_OuoDloXiCv",
        "outputId": "73a29915-6355-4111-a301-97116bee8285"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "small_subject = [\"상장지수펀드(ETF- Exchange Traded Fund)란?\", \"ETF 투자 핵심 전략 및 투자 방법\", \"ETF 투자의 매력과 주의할 점\"]\n",
            "contents = [[\"상장지수펀드는 특정 지수나 특정 자산의 수익률을 추적하는 펀드를 말합니다. 한국거래소에 상장되어 투자자들이 주식처럼 편리하게 매매할 수 있고, 개별 주식을 선택할 필요가 없어서 펀드 투자의 장점과 주식투자의 장점을 모두 가지고 있는 상품입니다.\"],\n",
            "            [\"ETF 투자의 핵심 전략 중 하나는 분할 매수 전략입니다. 이는 가치 있는 투자 대상의 경우 가격이 하락할 때 분할로 매수하여 평균 매입가를 낮출 수 있는 장점이 있습니다. 또 다른 전략으로는 코어-위성 전략이 있습니다. 이는 국가 산업 전체 지수를 추적하는 코어 ETF와 유망한 섹터 ETF를 조합하여 초과 수익을 추구하는 전략입니다. 마지막으로 로테이션 전략은 단기적인 주가 변동은 예측할 수 없지만, 거시적인 흐름과 시장 방향성은 예측할 수 있기 때문에 이를 활용하여 탄력적으로 운영하는 전략입니다.\"],\n",
            "            [\"ETF 투자의 매력은 운용보수가 저렴하고 현금화가 쉽다는 점입니다. 또한, 소액으로 분할 매수가 가능하고 다양한 투자가 가능하다는 점이 매력적입니다. 하지만 주의할 점은 시장 변동성으로 인해 원금 손실이 발생할 수 있다는 점과 해외 지수상품 ETF는 배당소득세가 부과된다는 점입니다. 따라서 ETF에 투자하기 전에 전문가와 충분한 상담을 진행하는 것이 좋습니다.\"]]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "exec(result)"
      ],
      "metadata": {
        "id": "65HIid9jzunh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_subject"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a0NDRML2UJN",
        "outputId": "d8fa15e7-7731-4f4b-f360-3b1c4f714b27"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['상장지수펀드(ETF- Exchange Traded Fund)란?',\n",
              " 'ETF 투자 핵심 전략 및 투자 방법',\n",
              " 'ETF 투자의 매력과 주의할 점']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chatbot"
      ],
      "metadata": {
        "id": "hOH1AZC2Xfqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "date = datetime.today().strftime(\"%Y/%m/%d\")"
      ],
      "metadata": {
        "id": "QuvHZR7rroX5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "17utwycTuuCp"
      },
      "outputs": [],
      "source": [
        "# 프롬프트를 'prompt_template'변수에 저장합니다.\n",
        "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "you are competent financial advisor. Today's date is August 18, 2023.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Don't answer with short answers, answer with complete sentences.\n",
        "Your answers should be detailed based on evidence, and don't say unnecessary things. Let's think step by step.\n",
        "Answer in Korean:\"\"\"\n",
        "# PROMPT에 'context'와 'question'을 입력 변수로하는 프롬프트 템플릿을 저장합니다.\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "# 'chain_type_kwargs'변수에 딕셔너리를 저장합니다.\n",
        "chain_type_kwargs = {\"prompt\": PROMPT}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "erFmJLbJRoq8"
      },
      "outputs": [],
      "source": [
        "# 'as_retriever'메서드를 호출하여 'vectordb'에서 생성된 벡터 데이터베이스를 검색 가능한 형태로 변환합니다.\n",
        "# 유사도가 높은 상위 3개의 문서만 반환하도록 지정합니다.\n",
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})\n",
        "# 'gpt-3.5-turbo-16k'모델을 사용합니다.\n",
        "# temperature값이 낮은수록 더 일관성 있는 문장이 생성됩니다.\n",
        "llm = ChatOpenAI(temperature=0.0,model_name='gpt-3.5-turbo-16k')\n",
        "\n",
        "# 체인타입을 결정합니다.\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    # 언어모델을 할당합니다.\n",
        "    llm=llm,\n",
        "    # 체인의 유형을 지정합니다.\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    # 응답과 함께 소스 문서를 반환합니다.\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs=chain_type_kwargs)\n",
        "\n",
        "# 언어 모델의 응답을 처리하는 함수를 정의합니다.\n",
        "def process_llm_response(llm_response):\n",
        "    # 언어 모델의 응답 결과를 출력합니다.\n",
        "    return(llm_response['result'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_YDUHNpRotL",
        "outputId": "17f070f8-1c17-494c-fc55-2d6bbb75caea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "작년은 2022년이었습니다. 작년 경제는 부진한 모습을 보였으며, 경기 침체와 기업 실적의 둔화가 예상되었습니다. 코스피 지수에 편입된 기업의 주당순이익은 지난해 17% 감소하였고, 올해도 3% 감소할 것으로 예상되었습니다. 그러나 내년에는 경기 여건의 개선으로 인해 주당순이익이 약 30% 증가할 것으로 전망되었습니다. 따라서 작년은 경제와 기업 실적이 부진한 상황이었으며, 이는 올해의 경제와 기업 실적에 영향을 미칠 수 있습니다.\n"
          ]
        }
      ],
      "source": [
        "query = \"작년은?\"\n",
        "answer = process_llm_response(llm_response = qa_chain(query))\n",
        "print(answer)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
